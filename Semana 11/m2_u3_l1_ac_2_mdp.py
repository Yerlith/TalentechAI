# -*- coding: utf-8 -*-
"""M2-U3-L1-Ac-2-MDP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T6dwCssGCX_lc8iM8ptD0TWM1cg_Lw0g
"""

import numpy as np
estados=['A','B','C',]
acciones=['arriba','abajo']
recompensas=np.random.randint(0,10,size=(len(estados),len(acciones)))


def transicion_aleatoria():
  return np.random.choice(estados)

estado_actual=np.random.choice(estados)
accion=np.random.choice(acciones)
nuevo_estado=transicion_aleatoria()
recompensa=recompensas[estados.index(estado_actual),acciones.index(accion)]
print(f"Estado actual: {estado_actual}")
print(f"Acci√≥n actual: {accion_actual}")
print(f"Nuevo estado: {nuevo_estado}")
print(f"Recompensa: {recompensa}")

def calcular_valor_estado(mdp,gamma=0.9,theta=0.01):
  valores={estado:0 for estado in mdp.estados}
  while True:
    delta=0
    for estado in mdp.estados:
      valor_previo=valores[estado]

      # calculated or provided for your MDP.
      # For demonstration, let's assume a simple transition model:
      valores[estado] = sum(1/len(mdp.estados) * (mdp.recompensas[mdp.estados.index(estado)][mdp.acciones.index(accion)] + gamma * valores[nuevo_estado])
                           for accion in mdp.acciones
                           for nuevo_estado in mdp.estados)
      delta=max(delta,abs(valor_previo-valores[estado]))
    if delta<theta:
      break
  return valores

# Define a sample 'mdp' object (replace with your actual MDP)
class MDP:
    def __init__(self):
        self.estados = ['A', 'B', 'C']
        self.acciones = ['arriba', 'abajo']
        self.recompensas = np.random.randint(0, 10, size=(len(self.estados), len(self.acciones)))
        # You would define 'transiciones' here in a real scenario

mdp = MDP() # Create an instance of the MDP class

valores_estados=calcular_valor_estado(mdp)

print(f"Valores de los estados:",valores_estados)

def verificar_propiedades(mdp):
  for estado in mdp.estados:
    for accion in mdp.acciones:
      suma_probabilidades = sum(mdp.transiciones[estado][accion].values())
      if not np.isclose(suma_probabilidades,1):
        return False
  return True

  print("Cumple con la propiedad de markov")
  verificar_propiedad_markov(mdp)

def calcular_recompensa_promedio(mdp):
  recompensa_total=0
  total_acciones=0
  for estado in mdp.estados:
    for accion in mdp.acciones:
      for nuevo_estado in mdp.estados: # Fixed typo here: mpd -> mdp
        recompensa_total+=mdp.recompensas[mdp.estados.index(estado)][mdp.acciones.index(accion)]
        total_acciones+=1 # Indented this line to be inside the innermost loop
  return recompensa_total/total_acciones

print("Recompensa promedio por accion:",calcular_recompensa_promedio(mdp))

